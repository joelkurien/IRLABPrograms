{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqo5Wg9oyYs1"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords, gutenberg\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BROTn_uKH3iZ",
        "outputId": "7bf6bea1-fb71-497f-957a-042a60ec8080"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42jdvS30H_9k",
        "outputId": "b35686a8-d33f-4489-ac8f-82903f9680dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nap8ol_fIQEu",
        "outputId": "7ef0c5fa-cf37-4950-ddd9-41b04de0143e"
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2NsilL7IkFm",
        "outputId": "8c0ccea7-1476-4f98-96fe-b65df1d61215"
      },
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOtYYZV-Ir7h"
      },
      "source": [
        "BLOCKSIZE=100000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXIkUrsVIjr"
      },
      "source": [
        "import re\n",
        "def symbolremoval(word):\n",
        "  return re.sub('[^A-Za-z0-9\\s]+', '', word).lower()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gU3w08rIxsU"
      },
      "source": [
        "from collections import defaultdict\n",
        "def bsbi():\n",
        "  termdoc_list = defaultdict(list)\n",
        "  with open('gutenberg_data.csv') as csv_file:\n",
        "    next(csv_file)\n",
        "    csv_files = csv.reader(csv_file)\n",
        "    fileno = 0\n",
        "    i = 0\n",
        "    current_block = 0\n",
        "    for row in csv_files:\n",
        "      #creating the term-DocID relation\n",
        "      title, author, link, id, bookshelf, body = row\n",
        "      i+=1\n",
        "      content = body.split()\n",
        "      for word in content:\n",
        "        word = symbolremoval(word)\n",
        "        if word and word not in stopWords:\n",
        "          word = porter.stem(word)\n",
        "          if word not in termdoc_list:\n",
        "            current_block += 1\n",
        "          \n",
        "          if id not in termdoc_list[word]:\n",
        "            termdoc_list[word].append(id)\n",
        "            current_block += 1\n",
        "          #sorting the term-DocID relation and creating its posting lists\n",
        "          if current_block >= BLOCKSIZE:\n",
        "            word_sorted = sorted(termdoc_list.items(), key=lambda x:x[0])\n",
        "            #creating block files\n",
        "            with open(f'./IRLAB/Lab2BLOCK{fileno}.txt', 'w') as myfile:\n",
        "              for id, docs in word_sorted:\n",
        "                myfile.write(id)\n",
        "                for docid in docs:\n",
        "                  myfile.write(f' {docid}')\n",
        "                myfile.write('\\n')\n",
        "            \n",
        "            current_block = 0\n",
        "            termdoc_list.clear()\n",
        "            fileno += 1\n",
        "    print(\"blocks computed\")    \n",
        "\n",
        "    wordsorted = sorted(termdoc_list.items(), key=lambda x:x[0])\n",
        "    if len(wordsorted)>0:\n",
        "      with open(f'./IRLAB/Lab2BLOCK{fileno}.txt', 'w') as myfile:\n",
        "        for id, docs in wordsorted:\n",
        "          myfile.write(id)\n",
        "          for docid in docs:\n",
        "            myfile.write(f' {docid}')\n",
        "          myfile.write('\\n')\n",
        "      current_block = 0\n",
        "      termdoc_list.clear()\n",
        "      fileno += 1     \n",
        "    print(i)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeZzw0mbJkYE"
      },
      "source": [
        "bsbi()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRv3bKPmdplQ"
      },
      "source": [
        "#find the block in the disk for adding into priority queue\n",
        "%cd \n",
        "file = [f'./IRLAB/Lab2BLOCK{i}.txt' for i in range(947)]\n",
        "fileseek = [open(i) for i in file]\n",
        "drive.mount('/content/drive')\n",
        "#merging the different block files created\n",
        "#into one merged file using heap queue\n",
        "import heapq\n",
        "\n",
        "#creating a generator such so as to split the words and their doc_ids \n",
        "#in the posting list of each word\n",
        "def decorated_file(f, key):\n",
        "  for word in f:\n",
        "    yield (key(word), word)\n",
        "  \n",
        "files = map(open, file)\n",
        "outfile = open('./IRLAB/Lab2merged.txt', 'w')\n",
        "\n",
        "#spliting the words\n",
        "def key_fn(word):\n",
        "  return word.split(' ',2)[0]\n",
        "\n",
        "wordpresent = ''\n",
        "for line in heapq.merge(*[decorated_file(f, key_fn) for f in files]):\n",
        "  #checking if the word already exists or not\n",
        "  if wordpresent != line[0]:\n",
        "    #if not add the new word to the file\n",
        "    outfile.write(f'\\n{line[1].strip()}')\n",
        "    wordpresent = line[0]\n",
        "  else:\n",
        "    #else append the new data into the word's list\n",
        "    outfile.write(f'{line[1][len(line[0]):].strip()}')\n",
        "for i in fileseek:\n",
        "  i.close()\n",
        "outfile.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOu0_JQcgpFD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}